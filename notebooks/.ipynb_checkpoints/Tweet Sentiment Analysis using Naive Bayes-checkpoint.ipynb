{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line to install dependencies\n",
    "# !pip install numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line to download the dataset\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we'll implement Naive Bayes classifier on twiiter sentiment dataset given in the `nltk` module.\n",
    "\n",
    "I will follow [this](https://virksaab.github.io/2020/08/20/worktree_mlcore_naivebayes_intro.html) explanation to implement the Naive Bayes. To train a naive bayes classifier, **the first step is to get the training and test dataset**. Let's import and load the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tweets: positive: 5000, negative: 5000\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "print(f\"# tweets: positive: {len(positive_tweets)}, negative: {len(negative_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training: 8000, 8000\n",
      "# Testing:  2000, 2000\n"
     ]
    }
   ],
   "source": [
    "split = 4000\n",
    "X_train = positive_tweets[:split] + negative_tweets[:split]\n",
    "X_test  = positive_tweets[split:] + negative_tweets[split:]\n",
    "\n",
    "# Along with the data, we need target values as well. \n",
    "# We will assume *positive tweets = 1* and *negative tweets = 0*\n",
    "y_train = [1]*(len(X_train)//2) + [0]*(len(X_train)//2)\n",
    "y_test = [1]*(len(X_test)//2) + [0]*(len(X_test)//2)\n",
    "\n",
    "print(f\"# Training: {len(X_train)}, {len(y_train)}\")\n",
    "print(f\"# Testing:  {len(X_test)}, {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Preprocess the tweets**\n",
    "\n",
    "After splitting the data, let's clean the data by removing noise, punctuations, stopwords, and hashtags.\n",
    "Along with cleaning, we will tokenize the tweet as well. Splitting words into units (here, substrings) from a sentence is called `Tokenization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: ['followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(tweet):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    cleaned_tweet = []\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            cleaned_tweet.append(word)\n",
    "    return cleaned_tweet\n",
    "\n",
    "# Run an example\n",
    "print(\"Example:\", preprocess(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Compute freq of words with respect to class table, freq(word, class)**\n",
    "\n",
    "It means count the number of times a word occurs in a class/label (0 for negative, 1 for positive).\n",
    "We will build a dictionary where the keys are a (word, class) and values are counts. Training data will be used for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [preprocess(tweet) for tweet in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_table(processed_tweets, labels): \n",
    "    freqs = {}\n",
    "    for tweet, label in zip(processed_tweets, labels):\n",
    "        for word in tweet:\n",
    "            if (word, label) in freqs.keys():\n",
    "                freqs[(word, label)] += 1\n",
    "            else:\n",
    "                freqs[(word, label)] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freq_table(processed_tweets, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
