{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**: Given a sentence, analysing whether the sentence is sad, happy, angry, etc. For example, _Woho! I got selected in Harvard university!_ reflects happy sentiment. _I lost my phone in subway_ reflects sad sentiment. Usually, we use tweets data for sentiment analysis, but, of course, you can use any data.\n",
    "\n",
    "We cannot use raw words as inputs to any machine learning model because data must be numerical. To convert text data into numerical, we use _vocabulary_.\n",
    "\n",
    "**Vocabulary**: It is a set of unique words that appear in the data. Usually, we build this vocab from training data, but we can use predefined vocabulary as well. For this tutorial, we'll make it from scratch. Vocabulary is required to create features from words and sentences to train a model. For example, _I lost my phone in the subway. I am stupid._ will build a vocabulary that contains I, lost, my, phone, in, subway, am, stupid. Now, there are several ways to create features from these words.\n",
    "\n",
    "There are several ways to convert text to numerical features. We will discuss them below.\n",
    "\n",
    "Watch [this video](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/gNXI3/vocabulary-feature-extraction) for more details on vocabulary and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start let's use a simple tweets dataset from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line to install dependencies\n",
    "# !pip install numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/virk/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "# uncomment below line to download the dataset\n",
    "# nltk.download('twitter_samples') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print('Number of positive tweets: ', len(positive_tweets))\n",
    "print('Number of negative tweets: ', len(negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive example -> #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Negative example -> hopeless for tmr :(\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example tweet\n",
    "print(\"Positive example ->\", positive_tweets[0])\n",
    "print()\n",
    "print(\"Negative example ->\", negative_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Vocabulary by splitting the string on space to get a list of words. This process is known as _tokenization_, and each word is known as _token_. Then we will take unique words from the training data to build vocabularyâ€”also, lowercase the data for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 116244 tokens.\n",
      "The vocab size is 26233.\n",
      "['', '@ark_yujin96', '@derekklahn', 'cases', 'experiencing', 'video', 'nae', '#decorating', '@thedemocrats', '@hackadayio']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "positive_tokens = []\n",
    "for sent in positive_tweets:\n",
    "    # Strip removes trailing white space from string ends.\n",
    "    positive_tokens += sent.lower().strip().split(\" \")\n",
    "negative_tokens = []\n",
    "for sent in negative_tweets:\n",
    "    # Strip removes trailing white space from string ends.\n",
    "    negative_tokens += sent.lower().strip().split(\" \")\n",
    "    \n",
    "# Combine all tokens\n",
    "tokens = positive_tokens + negative_tokens\n",
    "print(f\"There are total {len(tokens)} tokens.\")\n",
    "# Now let's take unique words only to build a vocab\n",
    "vocab = list(set(tokens))\n",
    "print(f\"The vocab size is {len(vocab)}.\")\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a vocab, and we have to figure out how to create features from these words. One way is one-hot vector using word's index from vocab. A vector in python is simply a list of words of a particular sentence.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample: Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/xxlXs6xYwe.\n",
      "number of words/tokens in this example: 15\n",
      "[6820, 23233, 22609, 12793, 14574, 23682, 10528, 8235, 12313, 2170, 17006, 888, 14918, 9005, 15061]\n"
     ]
    }
   ],
   "source": [
    "sample = positive_tweets[300]\n",
    "print(f\"Original Sample: {sample}\")\n",
    "words = [vocab.index(word) for word in sample.lower().strip().split(\" \")]\n",
    "print(\"number of words/tokens in this example:\", len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our one-hot vector will be of a size equal to the size of vocab, i.e. 26233 in this case, and we will one-hot 15 words at given indices in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of sample vector: (26233,)\n",
      "Counter({0: 26218, 1: 15})\n"
     ]
    }
   ],
   "source": [
    "sample_vector = np.zeros(len(vocab), dtype=int)\n",
    "print(\"Size of sample vector:\", sample_vector.shape)\n",
    "# Make one-hot\n",
    "sample_vector[words] = 1.\n",
    "# Verify by counting\n",
    "print(Counter(sample_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to arrange the samples and split the data for training and validation. I am using pandas and sklearn here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5000' class='' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5000/5000 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5000' class='' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5000/5000 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use vocab to convert sentences to one-hot vectors\n",
    "positive_tweets_vectors = []\n",
    "for sent in progress_bar(positive_tweets):\n",
    "    words = [vocab.index(word) for word in sent.lower().strip().split(\" \")]\n",
    "    sent_vec = np.zeros(len(vocab), dtype=int)\n",
    "    sent_vec[words] = 1.\n",
    "    positive_tweets_vectors.append(sent_vec)\n",
    "negative_tweets_vectors = []\n",
    "for sent in progress_bar(negative_tweets):\n",
    "    words = [vocab.index(word) for word in sent.lower().strip().split(\" \")]\n",
    "    sent_vec = np.zeros(len(vocab), dtype=int)\n",
    "    sent_vec[words] = 1.\n",
    "    negative_tweets_vectors.append(sent_vec)\n",
    "\n",
    "positive_tweets_vectors = np.array(positive_tweets_vectors)\n",
    "negative_tweets_vectors = np.array(negative_tweets_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data and features: (10000, 26234)\n"
     ]
    }
   ],
   "source": [
    "posdf = pd.DataFrame(positive_tweets_vectors)\n",
    "posdf[\"target\"] = 1\n",
    "negdf = pd.DataFrame(negative_tweets_vectors)\n",
    "negdf[\"target\"] = 0\n",
    "df = pd.concat([posdf, negdf])\n",
    "print(\"Size of data and features:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with machine learning algorithms, then you already know that this is a massive number of features and it will be computationally more expensive for bigger datasets. Anyway, we will try a logistic regression model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training and validation: (7500, 26234) (2500, 26234)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation\n",
    "traindf, valdf = train_test_split(df, shuffle=True, stratify=df.target)\n",
    "print(f\"Number of samples in training and validation:\", traindf.shape, valdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    3750\n",
       " 0    3750\n",
       " Name: target, dtype: int64,\n",
       " 1    1250\n",
       " 0    1250\n",
       " Name: target, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the classes split\n",
    "traindf.target.value_counts(), valdf.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 26233) (7500,) (2500, 26233) (2500,)\n",
      "training accuracy: 1.0\n",
      "CPU times: user 21.6 s, sys: 2.57 s, total: 24.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train model\n",
    "X_train, y_train = traindf.drop(\"target\", axis=1).values, traindf.target.values\n",
    "X_val, y_val = valdf.drop(\"target\", axis=1).values, valdf.target.values\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"training accuracy:\", clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation accuracy:\", clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!! That's really impressive results with a simple logistic regression. Let's test it with our made-up sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sent):\n",
    "    # Convert to one-hot vector\n",
    "    words = [vocab.index(word) for word in sent.lower().strip().split(\" \")]\n",
    "    sent_vec = np.zeros(len(vocab), dtype=int)\n",
    "    sent_vec[words] = 1.\n",
    "    sent_vec = sent_vec.reshape(1, -1)\n",
    "    # predict\n",
    "    y_pred = clf.predict(sent_vec)\n",
    "    if y_pred[0] == 1: return \"positive\"\n",
    "    elif y_pred[0] == 0: return \"negative\"\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I am happy about the results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"This worked out fine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I lost my phone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"Julia broke up with John\") # What? that's not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I forgot my lunch at home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I'm sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"this is a sick beat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"Get away from me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this worked out well. However, there are some issues:\n",
    "* The model took ~12GB RAM while training.\n",
    "* We need another method to create features because one-hot will get expensive with large datasets.\n",
    "* We still need to handle words that are not in the vocab which will raise the below error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'tutorial' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4607a7e11912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This tutorial worked out well :)\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Damn it :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-87f8bb73d130>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Convert to one-hot vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msent_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msent_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-87f8bb73d130>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Convert to one-hot vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msent_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msent_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'tutorial' is not in list"
     ]
    }
   ],
   "source": [
    "test(\"This tutorial worked out well :)\") # Damn it :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- Natual Language Processing Specialization - Course 1 - Week 1 - Coursera"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
