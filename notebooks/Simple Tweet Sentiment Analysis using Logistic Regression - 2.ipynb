{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second tutorial of _Tweet Sentiment Analysis with Logistic Regression_ series. Check out the first section if you haven't yet. \n",
    "In this tutorial, we will use another method to create features from sentences. This method is _frequency count_. This method creates only 3 features rather than 26233 using one-hot!\n",
    "\n",
    "**Frequency count**: Number of times a word appear in particular class corpus. Watch [this video](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/cITmZ/negative-and-positive-frequencies) for clear explanation. After that, watch [this video](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/lecture/j92dt/feature-extraction-with-frequencies) for feature extraction using word frequencies.\n",
    "\n",
    "The data loading process is the same as previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line to install dependencies\n",
    "# !pip install numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/virk/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "# uncomment below line to download the dataset\n",
    "nltk.download('twitter_samples') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print('Number of positive tweets: ', len(positive_tweets))\n",
    "print('Number of negative tweets: ', len(negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive example -> #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Negative example -> hopeless for tmr :(\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example tweet\n",
    "print(\"Positive example ->\", positive_tweets[0])\n",
    "print()\n",
    "print(\"Negative example ->\", negative_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a pandas dataframe for easy data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdf = pd.DataFrame(positive_tweets, columns=[\"tweet\"])\n",
    "posdf[\"target\"] = 1\n",
    "negdf = pd.DataFrame(negative_tweets, columns=[\"tweet\"])\n",
    "negdf[\"target\"] = 0\n",
    "# Combine both dataframes\n",
    "df = pd.concat([posdf, negdf])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>@bmthofficial They are all sold out :((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>@meliefluous pamer? :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>@DEPORSEMPRE1 hello, any info about possible i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>@milay_44 yeah :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>@LoLEsportspedia thanks :D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>I wanna buy the #calibraskaEP :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  target\n",
       "2455          @bmthofficial They are all sold out :((((       0\n",
       "1916                             @meliefluous pamer? :)       1\n",
       "4255  @DEPORSEMPRE1 hello, any info about possible i...       0\n",
       "3071                                  @milay_44 yeah :)       1\n",
       "80                           @LoLEsportspedia thanks :D       1\n",
       "2430                   I wanna buy the #calibraskaEP :(       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some cleaning such as converting all text to lowercase, remove hashtags, extra spaces, etc. For the sake of simplicity, I am performing simple cleaning only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tweet):\n",
    "    # lowercase\n",
    "    tweet = tweet.lower().strip()\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>miss u :-( @deepikapadukone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>@gufuus i saw one doujin i think of killua get...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>@silv3r i can also provide this, just pop over...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hopeless for tmr :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>final ep of got, here we go. :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>your happiness is your responsibilty. so, don'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  target\n",
       "3347                        miss u :-( @deepikapadukone       0\n",
       "4461  @gufuus i saw one doujin i think of killua get...       0\n",
       "3825  @silv3r i can also provide this, just pop over...       1\n",
       "0                                   hopeless for tmr :(       0\n",
       "1181                    final ep of got, here we go. :(       0\n",
       "4752  your happiness is your responsibilty. so, don'...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet\"] = df.tweet.apply(preprocessing)\n",
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataframe and split the data into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train and val set: (7500, 2) (2500, 2)\n",
      "Samples distribution in train set: {0: 3777, 1: 3723}\n",
      "Samples distribution in val set: {1: 1277, 0: 1223}\n"
     ]
    }
   ],
   "source": [
    "traindf, valdf = train_test_split(df, shuffle=True)\n",
    "print(\"Shape of train and val set:\", traindf.shape, valdf.shape)\n",
    "# Verify the classes of both splits\n",
    "print(\"Samples distribution in train set:\", dict(traindf.target.value_counts()))\n",
    "print(\"Samples distribution in val set:\", dict(valdf.target.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build word frequencies using training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs_dict(df):\n",
    "    freqs = {}\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        y = row.target\n",
    "        for word in row.tweet.split(\" \"):\n",
    "            pair = (word, y)\n",
    "            \n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "            # The above line is equivalent to if else below but compact\n",
    "#             if pair in freqs: freqs[pair] += 1\n",
    "#             else: freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 6.94 ms, total: 1.17 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freqs = build_freqs_dict(traindf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make features from the frequencies...\n",
    "There will be three features for every tweet:\n",
    "* Bias: 1 for all tweets\n",
    "* positive frequencies count: Number of words present in freqs dict with target = 1 or positive\n",
    "* negative frequencies count: Number of words present in freqs dict with target = 0 or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(tweet, freqs):\n",
    "    # Initialize a zeros array with size 3\n",
    "    feats = np.zeros(3, dtype=int)\n",
    "    # set bias term to 1\n",
    "    feats[0] = 1\n",
    "    \n",
    "    for word in tweet.split(\" \"):\n",
    "        # Set positive frequencies count\n",
    "        if (word, 1) in freqs.keys(): feats[1] += freqs[(word, 1)]\n",
    "        # Set negative frequencies count\n",
    "        if (word, 0) in freqs.keys(): feats[2] += freqs[(word, 0)]\n",
    "    \n",
    "    assert(feats.shape == (3,))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet: there's nothing as cool as being totally over someone, no bitterness, anger or hatred towards them. just pure indifference :)\n",
      "Sample features: [   1 2945  622]\n"
     ]
    }
   ],
   "source": [
    "# Test make_features function\n",
    "sample = traindf.tweet.iloc[10]\n",
    "sample_feats = make_features(sample, freqs)\n",
    "print(\"Sample tweet:\", sample)\n",
    "print(\"Sample features:\", sample_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make features and train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 3) (7500,) (2500, 3) (2500,)\n"
     ]
    }
   ],
   "source": [
    "X_train = traindf.tweet.apply(partial(make_features, freqs=freqs))\n",
    "X_train = np.stack(X_train.values)\n",
    "y_train = traindf.target.values\n",
    "\n",
    "X_val = valdf.tweet.apply(partial(make_features, freqs=freqs))\n",
    "X_val = np.stack(X_val.values)\n",
    "y_val = valdf.target.values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9328\n",
      "CPU times: user 91.2 ms, sys: 7.88 ms, total: 99.1 ms\n",
      "Wall time: 44.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation accuracy:\", clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are acceptable results. However, we can improve our model by stemming and removing stop words and punctuation.\n",
    "\n",
    "Let's do it...\n",
    "\n",
    "We already lowercased and removed hastags. Now, we will perform:\n",
    "* removing hyperlinks\n",
    "* using a more sophisticated tokenizer instead of `.split` method\n",
    "* remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/virk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "from string import punctuation             # common punctuations\n",
    "\n",
    "# uncomment below 2 lines to download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing2(tweet, tokenizer):\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\\\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # tokenize\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    # remove stop words and punctuations\n",
    "    clean_tweet = [ # Go through every word in tokens list\n",
    "        word for word in tweet\n",
    "        if ((word not in stopwords_english) and (word not in punctuation))\n",
    "    ]\n",
    "    # Make a string from tokens\n",
    "    clean_tweet = \" \".join(clean_tweet)\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 79.8 ms, total: 1.74 s\n",
      "Wall time: 1.22 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>happy friday checking see enjoyed super-blend :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>kik thenting 423 kik kiksex omegle skype amate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>burned pizza rolls :-(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>dear last night waited 2hrs 13m pizza time arr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>omg can't believe vampire diaries followed tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>absolutely waiting day :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  target\n",
       "352    happy friday checking see enjoyed super-blend :)       1\n",
       "732   kik thenting 423 kik kiksex omegle skype amate...       0\n",
       "2121                             burned pizza rolls :-(       0\n",
       "3562  dear last night waited 2hrs 13m pizza time arr...       0\n",
       "2799  omg can't believe vampire diaries followed tha...       1\n",
       "1248                          absolutely waiting day :)       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df[\"tweet\"] = df.tweet.apply(partial(preprocessing2, tokenizer=tokenizer))\n",
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data, build freqs, and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train and val set: (8000, 2) (2000, 2)\n",
      "Samples distribution in train set: {0: 4005, 1: 3995}\n",
      "Samples distribution in val set: {1: 1005, 0: 995}\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "traindf, valdf = train_test_split(df, shuffle=True, test_size=.20)\n",
    "print(\"Shape of train and val set:\", traindf.shape, valdf.shape)\n",
    "# Verify the classes of both splits\n",
    "print(\"Samples distribution in train set:\", dict(traindf.target.value_counts()))\n",
    "print(\"Samples distribution in val set:\", dict(valdf.target.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 0 ns, total: 1.11 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Make freqs\n",
    "freqs = build_freqs_dict(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 3) (8000,) (2000, 3) (2000,)\n",
      "CPU times: user 207 ms, sys: 3.15 ms, total: 210 ms\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Make features\n",
    "X_train = traindf.tweet.apply(partial(make_features, freqs=freqs))\n",
    "X_train = np.stack(X_train.values)\n",
    "y_train = traindf.target.values\n",
    "\n",
    "X_val = valdf.tweet.apply(partial(make_features, freqs=freqs))\n",
    "X_val = np.stack(X_val.values)\n",
    "y_val = valdf.target.values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 3.99 ms, total: 164 ms\n",
      "Wall time: 42.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train and test\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.990625\n",
      "Validation accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Validation accuracy:\", clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WoW! we gain a significant improvement with better preprocessing/\n",
    "\n",
    "Now. let's test our model with our text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sent):\n",
    "    sent = preprocessing2(sent, tokenizer)\n",
    "    test_feats = make_features(sent, freqs)\n",
    "    y_pred = clf.predict(test_feats.reshape(1,-1))\n",
    "    if y_pred[0] == 1: return \"positive\"\n",
    "    elif y_pred[0] == 0: return \"negative\"\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I am happy about the results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"This worked out fine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(test(\"I lost my phone.\")) # This should be negative\n",
    "# Why this is positive?\n",
    "print(test(\"lost\")) # returns positive.\n",
    "# This might be because the word \"lost\" is present in the positive freqs for some reason.\n",
    "# This could be a drawback of using word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"Julia broke up with John\") # What? that's not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I forgot my lunch at home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"I'm sick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"this is a sick beat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"Get away from me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So _frequencies count_ method is significantly faster and more accurate (at least in validation set). However, further insights are required. Also, we might need more complex features for difficult datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pt)",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
